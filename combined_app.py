#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
combined_app.py

ç›®çš„:
  app.pyã¨main.pyã‚’çµ±åˆã—ãŸã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦å‰‡ãƒã‚§ãƒƒã‚¯RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ
  CUIãƒ¢ãƒ¼ãƒ‰ã¨Streamlit WebUIã®ä¸¡æ–¹ã‚’ã‚µãƒãƒ¼ãƒˆã—ã¾ã™ã€‚

çµ±åˆæ©Ÿèƒ½:
  - ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã§CUIãƒ¢ãƒ¼ãƒ‰ï¼ˆ--cuiï¼‰ã¾ãŸã¯WebUIãƒ¢ãƒ¼ãƒ‰ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼‰ã‚’é¸æŠ
  - ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦å‰‡ä¸€è¦§ãƒ«ãƒ¼ãƒ«ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ AgentRAG
  - ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ç´„ãƒ»ç¢ºèªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆï¼ˆAgent A/Bï¼‰
  - LCELï¼ˆRunnable ç­‰ï¼‰ã‚’åˆ©ç”¨ã—ãŸãƒã‚§ãƒ¼ãƒ³
  - OpenAI ã® gpt-4o ã‚’ ChatOpenAI ã§å‘¼ã³å‡ºã—ï¼ˆOPENAI_API_KEY å¿…é ˆï¼‰

ä½¿ç”¨æ–¹æ³•:
  python combined_app.py           # Streamlit WebUI ãƒ¢ãƒ¼ãƒ‰
  python combined_app.py --cui     # CUI ãƒ¢ãƒ¼ãƒ‰
"""

import os
import sys
import json
import glob
import re
from pathlib import Path
from typing import List, Dict, Any, Optional, Tuple
import textwrap
from datetime import datetime
import argparse

# --- ä¾å­˜ãƒ‘ãƒƒã‚±ãƒ¼ã‚¸ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨ç’°å¢ƒã§ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹å‰æï¼‰ ---
try:
    # LangChain ã¨ãƒ¢ãƒ‡ãƒ«ãƒ©ãƒƒãƒ‘ãƒ¼
    from langchain.chat_models import ChatOpenAI
    from langchain.schema import HumanMessage, SystemMessage
    from langchain.embeddings import HuggingFaceEmbeddings
    from langchain.vectorstores import Chroma
    from langchain.text_splitter import CharacterTextSplitter
    from langchain.docstore.document import Document
except Exception as e:
    print("å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: langchain ç­‰ã€‚\n`pip install -r requirements.txt` ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚\nã‚¨ãƒ©ãƒ¼: ", e)
    sys.exit(1)

# LCEL ç³»ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆï¼ˆåˆ©ç”¨å¯èƒ½ãªã‚‰åˆ©ç”¨ã™ã‚‹ï¼‰
USE_LCEL = True
import importlib

Runnable = None
RunnablePassthrough = None
try:
    mod = importlib.import_module("langchain_experimental")
    Runnable = getattr(mod, "Runnable", None)
    RunnablePassthrough = getattr(mod, "RunnablePassthrough", None)
except Exception:
    try:
        mod2 = importlib.import_module("langchain.experimental.runnable")
        Runnable = getattr(mod2, "Runnable", None)
        RunnablePassthrough = getattr(mod2, "RunnablePassthrough", None)
    except Exception:
        Runnable = None
        RunnablePassthrough = None

if Runnable is None or RunnablePassthrough is None:
    USE_LCEL = False

import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# ç’°å¢ƒå¤‰æ•°ã‹ã‚‰ API ã‚­ãƒ¼ã‚’å–å¾—
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    print("ç’°å¢ƒå¤‰æ•° OPENAI_API_KEY ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚è¨­å®šã—ã¦ã‹ã‚‰å†å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    sys.exit(1)

# --- è¨­å®šå€¤ ---
BASE_DIR = Path(__file__).parent
RULE_DIR = BASE_DIR / "rule"
SPEC_DIR = BASE_DIR / "specification"
CHROMA_DIR = BASE_DIR / "chroma_db"
CHROMA_COLLECTION = "specs"

# Embedding ãƒ¢ãƒ‡ãƒ«åï¼ˆãƒ­ãƒ¼ã‚«ãƒ«ã§å®Ÿè¡Œã§ãã‚‹è»½é‡ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ï¼‰
EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"

# RAG æ¤œç´¢æ™‚ã«å–ã‚Šå‡ºã™ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°
TOP_K = 3


def load_rules_from_dir(rule_dir: Path) -> List[Dict[str, Any]]:
    """rule ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªé…ä¸‹ã®å…¨ JSON ã‚’å†å¸°çš„ã«èª­ã¿è¾¼ã¿ã€ãƒ«ãƒ¼ãƒ«ã‚’ãƒ•ãƒ©ãƒƒãƒˆãªãƒªã‚¹ãƒˆã§è¿”ã™ã€‚

    ãƒ«ãƒ¼ãƒ« JSON ã¯é…åˆ— ã¾ãŸã¯ {"rules": [...] } ã®å½¢å¼ã«å¯¾å¿œã€‚
    å†å¸°çš„ãªæ§‹é€ ï¼ˆå­ãƒ«ãƒ¼ãƒ«ã‚’ 'children' ãªã©ã§æŒã¤ï¼‰ã‚‚ãƒ•ãƒ©ãƒƒãƒˆåŒ–ã—ã¦è¿”ã™ã€‚
    
    æ–°æ©Ÿèƒ½: touitsukijun_r7.json ã®éšå±¤æ§‹é€  (sections -> subsections -> items) ã«å¯¾å¿œ
    """
    rules: List[Dict[str, Any]] = []
    
    def create_rule_entry(item: Dict[str, Any], parent_path: str = "", file_source: str = "") -> Dict[str, Any]:
        """å€‹åˆ¥ãƒ«ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒªã‚’ä½œæˆ"""
        rid = item.get("id") or item.get("rule_id") or item.get("name") or None
        title = item.get("title") or item.get("name") or rid or "unnamed"
        path_label = f"{parent_path}/{title}" if parent_path else title
        
        # content æ§‹ç¯‰: description + information (ã‚ã‚Œã°)
        content_parts = []
        if item.get("description"):
            content_parts.append(item["description"])
        if item.get("information"):
            content_parts.append(f"\n[è©³ç´°æƒ…å ±]\n{item['information']}")
        if item.get("content"):
            content_parts.append(item["content"])
            
        content = "\n".join(content_parts) if content_parts else json.dumps(item, ensure_ascii=False)
        
        return {
            "id": rid,
            "title": title,
            "path": path_label,
            "content": content,
            "type": item.get("type", "æœªåˆ†é¡"),
            "source_file": file_source,
            "raw": item,
        }
    
    for path in rule_dir.rglob("*.json"):
        try:
            with open(path, "r", encoding="utf-8") as f:
                data = json.load(f)
        except Exception as e:
            logger.warning(f"ãƒ«ãƒ¼ãƒ«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã‚ã¾ã›ã‚“ã§ã—ãŸ: {path} - {e}")
            continue

        file_source = path.name
        
        # å…¥ã‚Œå­å¯¾å¿œ: data ãŒé…åˆ—ã‹è¾æ›¸ã‹
        candidates = []
        if isinstance(data, list):
            candidates = data
        elif isinstance(data, dict):
            if "rules" in data and isinstance(data["rules"], list):
                candidates = data["rules"]
            else:
                candidates = [data]

        # éšå±¤æ§‹é€ ã®å†å¸°çš„å‡¦ç†
        def walk(item: Dict[str, Any], parent_path: str = ""):
            # ç¾åœ¨ã®é …ç›®ãŒãƒ«ãƒ¼ãƒ«ã¨ã—ã¦è¿½åŠ ã™ã¹ãã‚‚ã®ã‹ãƒã‚§ãƒƒã‚¯
            has_description = bool(item.get("description") or item.get("content"))
            
            if has_description:
                # ãƒ«ãƒ¼ãƒ«ã‚¨ãƒ³ãƒˆãƒªã¨ã—ã¦è¿½åŠ 
                entry = create_rule_entry(item, parent_path, file_source)
                rules.append(entry)
            
            # éšå±¤ã‚’ä¸‹ã£ã¦å­è¦ç´ ã‚’å‡¦ç†
            current_path = f"{parent_path}/{item.get('title', item.get('id', ''))}" if parent_path else (item.get('title') or item.get('id') or "")
            
            # æ–°ã—ã„éšå±¤ã‚­ãƒ¼: sections, subsections, items ã«å¯¾å¿œ
            for child_key in ("children", "rules", "subrules", "items", "sections", "subsections"):
                if child_key in item and isinstance(item[child_key], list):
                    for child in item[child_key]:
                        if isinstance(child, dict):
                            walk(child, current_path)

        for it in candidates:
            if isinstance(it, dict):
                walk(it)

    logger.info(f"èª­ã¿è¾¼ã‚“ã ãƒ«ãƒ¼ãƒ«æ•°: {len(rules)}")
    return rules


def text_from_pdf(path: Path) -> str:
    """ã‚·ãƒ³ãƒ—ãƒ«ãª PDF ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡ºã€‚pypdf ã‚’åˆ©ç”¨ã€‚ãƒšãƒ¼ã‚¸ã”ã¨ã«é€£çµã™ã‚‹ã€‚"""
    try:
        import pypdf
    except Exception:
        raise RuntimeError("pypdf ãŒå¿…è¦ã§ã™ã€‚pip install pypdf ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    text_parts = []
    try:
        reader = pypdf.PdfReader(str(path))
        for p in reader.pages:
            txt = p.extract_text() or ""
            text_parts.append(txt)
    except Exception as e:
        logger.warning(f"PDF èª­ã¿è¾¼ã¿å¤±æ•— {path}: {e}")
    return "\n".join(text_parts)


def text_from_docx(path: Path) -> str:
    try:
        import docx
    except Exception:
        raise RuntimeError("python-docx ãŒå¿…è¦ã§ã™ã€‚pip install python-docx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    try:
        doc = docx.Document(str(path))
        return "\n".join(p.text for p in doc.paragraphs)
    except Exception as e:
        logger.warning(f"DOCX èª­ã¿è¾¼ã¿å¤±æ•— {path}: {e}")
        return ""


def text_from_xlsx(path: Path) -> str:
    """Excel ãƒ•ã‚¡ã‚¤ãƒ«(.xlsx)ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        import openpyxl
    except Exception:
        raise RuntimeError("openpyxl ãŒå¿…è¦ã§ã™ã€‚pip install openpyxl ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    
    text_parts = []
    try:
        workbook = openpyxl.load_workbook(str(path), read_only=True, data_only=True)
        for sheet_name in workbook.sheetnames:
            sheet = workbook[sheet_name]
            text_parts.append(f"\n--- ã‚·ãƒ¼ãƒˆ: {sheet_name} ---\n")
            
            for row in sheet.iter_rows(values_only=True):
                row_text = []
                for cell in row:
                    if cell is not None:
                        row_text.append(str(cell))
                if row_text:
                    text_parts.append("\t".join(row_text))
        workbook.close()
    except Exception as e:
        logger.warning(f"XLSX èª­ã¿è¾¼ã¿å¤±æ•— {path}: {e}")
        return ""
    
    return "\n".join(text_parts)


def text_from_pptx(path: Path) -> str:
    """PowerPoint ãƒ•ã‚¡ã‚¤ãƒ«(.pptx)ã‹ã‚‰ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
    try:
        from pptx import Presentation
    except Exception:
        raise RuntimeError("python-pptx ãŒå¿…è¦ã§ã™ã€‚pip install python-pptx ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
    
    text_parts = []
    try:
        prs = Presentation(str(path))
        for i, slide in enumerate(prs.slides, 1):
            text_parts.append(f"\n--- ã‚¹ãƒ©ã‚¤ãƒ‰ {i} ---\n")
            
            # ã‚¹ãƒ©ã‚¤ãƒ‰å†…ã®å…¨ãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º
            for shape in slide.shapes:
                if hasattr(shape, "text") and shape.text.strip():
                    text_parts.append(shape.text.strip())
                
                # è¡¨ãŒã‚ã‚‹å ´åˆã®ãƒ†ã‚­ã‚¹ãƒˆæŠ½å‡º
                if shape.has_table:
                    table = shape.table
                    for row in table.rows:
                        row_text = []
                        for cell in row.cells:
                            if cell.text.strip():
                                row_text.append(cell.text.strip())
                        if row_text:
                            text_parts.append("\t".join(row_text))
    except Exception as e:
        logger.warning(f"PPTX èª­ã¿è¾¼ã¿å¤±æ•— {path}: {e}")
        return ""
    
    return "\n".join(text_parts)


def load_spec_documents(spec_dir: Path) -> List[Document]:
    """`specification/` é…ä¸‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’èª­ã¿è¾¼ã¿ã€langchain Document ã®ãƒªã‚¹ãƒˆã‚’è¿”ã™ã€‚

    å¯¾å¿œ: pdf, docx, xlsx, pptx, md, txt
    ãƒ¡ãƒ¢ãƒªç¯€ç´„: ãƒ•ã‚¡ã‚¤ãƒ«æ¯ã«ãƒãƒ£ãƒ³ã‚¯åˆ†å‰²ã‚’è¡Œã„ã€æœ€ä½é™ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’ä»˜ä¸
    """
    docs: List[Document] = []
    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    for path in spec_dir.rglob("*"):
        if path.is_dir():
            continue
        lower = path.suffix.lower()
        try:
            if lower == ".pdf":
                text = text_from_pdf(path)
            elif lower == ".docx":
                text = text_from_docx(path)
            elif lower == ".xlsx":
                text = text_from_xlsx(path)
            elif lower == ".pptx":
                text = text_from_pptx(path)
            elif lower in (".md", ".txt"):
                text = path.read_text(encoding="utf-8", errors="ignore")
            else:
                logger.debug(f"æœªå¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼ã‚’ã‚¹ã‚­ãƒƒãƒ—: {path}")
                continue
        except Exception as e:
            logger.warning(f"ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿å¤±æ•— {path}: {e}")
            continue

        if not text.strip():
            logger.debug(f"ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ã‚¹ã‚­ãƒƒãƒ—: {path}")
            continue

        chunks = text_splitter.split_text(text)
        for i, c in enumerate(chunks):
            meta = {"source": str(path), "chunk": i, "file_type": lower}
            docs.append(Document(page_content=c, metadata=meta))

    logger.info(f"èª­ã¿è¾¼ã‚“ã ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãƒãƒ£ãƒ³ã‚¯æ•°: {len(docs)}")
    return docs


def init_chroma(docs: List[Document]) -> Chroma:
    """ChromaDB ã‚’åˆæœŸåŒ–ã—ã¦ã€ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ ¼ç´ã™ã‚‹ã€‚æ—¢å­˜ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ãŒã‚ã‚Œã°å†åˆ©ç”¨ã€‚

    åŸ‹ã‚è¾¼ã¿ã¯ãƒ­ãƒ¼ã‚«ãƒ« HuggingFace ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ã†ï¼ˆè»½é‡ãƒ¢ãƒ‡ãƒ«æ¨å¥¨ï¼‰
    """
    embeddings = HuggingFaceEmbeddings(model_name=EMBEDDING_MODEL)
    vectordb = Chroma(persist_directory=str(CHROMA_DIR), collection_name=CHROMA_COLLECTION, embedding_function=embeddings)

    try:
        existing = vectordb._collection.count() if hasattr(vectordb, "_collection") else None
    except Exception:
        existing = None

    if existing in (None, 0):
        if docs:
            logger.info("Chroma ã«ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’è¿½åŠ ã—ã¾ã™...")
            vectordb.add_documents(docs)
            vectordb.persist()
    else:
        logger.info("æ—¢å­˜ã® Chroma ã‚³ãƒ¬ã‚¯ã‚·ãƒ§ãƒ³ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚æ–°è¦è¿½åŠ ã¯è¡Œã„ã¾ã›ã‚“ã€‚")

    return vectordb


def retrieve_related_docs(vectordb: Chroma, query: str, k: int = TOP_K, include_rules: bool = False) -> List[Document]:
    """é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã™ã‚‹ã€‚"""
    docs = []
    
    try:
        main_docs = vectordb.similarity_search(query, k=k)
        docs.extend(main_docs)
    except Exception as e:
        logger.warning(f"ãƒ¡ã‚¤ãƒ³ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ¤œç´¢ã‚¨ãƒ©ãƒ¼: {e}")
    
    return docs[:k]


def make_chat_model() -> ChatOpenAI:
    """ChatOpenAI ã‚’ä½œæˆã€‚ãƒ¢ãƒ‡ãƒ«åã¯ gpt-4o ã‚’æŒ‡å®šã™ã‚‹ã€‚"""
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.0)
    return llm


def agent_a_summarize(llm: ChatOpenAI, rule_text: str, docs: List[Document]) -> str:
    """Agent A: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ç´„ã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    context = "\n\n---é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ---\n"
    for d in docs[:TOP_K]:
        src = d.metadata.get("source") if d.metadata else "<unknown>"
        context += f"[source: {src}]\n{d.page_content}\n\n"

    system_prompt = (
        "ã‚ãªãŸã¯å„ªç§€ãªãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆè¦ç´„è€…ã§ã™ã€‚ä»¥ä¸‹ã®ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£ãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿ã€"
        "é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’å®Œå…¨æ€§ã‚’ä¿ã£ã¦è¦ç´„ã—ã¦ãã ã•ã„ã€‚é‡è¦ãªæ¡ä»¶ã€è¦ä»¶ã€åŠã³æ¤œè¨¼ãƒã‚¤ãƒ³ãƒˆã‚’ç®‡æ¡æ›¸ãã§ç¤ºã—ã¦ãã ã•ã„ã€‚"
        "\n\né‡è¦: å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚å‡ºåŠ›ã®æœ¬æ–‡ã¯æ—¥æœ¬èªã§è¨˜è¼‰ã—ã€èª­ã¿ã‚„ã™ã„ç®‡æ¡æ›¸ãã‚’å¿ƒãŒã‘ã¦ãã ã•ã„ã€‚"
    )
    messages = [SystemMessage(content=system_prompt), HumanMessage(content=f"ãƒ«ãƒ¼ãƒ«:\n{rule_text}\n\nãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}")]
    resp = llm(messages)
    return resp.content


def agent_b_check(llm: ChatOpenAI, rule_summary: str, rule_raw: Dict[str, Any], docs: List[Document]) -> Dict[str, Any]:
    """Agent B: ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆç¢ºèªã‚¨ãƒ¼ã‚¸ã‚§ãƒ³ãƒˆ"""
    context = "\n\n".join([f"[src:{d.metadata.get('source')}]\n{d.page_content}" for d in docs[:TOP_K]])

    strict_prompt = """
ã‚ãªãŸã¯æŠ€è¡“çš„ãªè©•ä¾¡è€…ã§ã™ã€‚ä»¥ä¸‹ã®ãƒ«ãƒ¼ãƒ«è¦ç´„ã¨å…ƒãƒ«ãƒ¼ãƒ«ã‚’èª­ã¿ã€ä¸ãˆã‚‰ã‚ŒãŸã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ãŒãã®ãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã„ã‚‹ã‹ã‚’è©•ä¾¡ã—ã¦ãã ã•ã„ã€‚

å‡ºåŠ›ã¯å³å¯†ãª JSON ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚ä½™è¨ˆãªèª¬æ˜ã‚„è¿½åŠ ãƒ†ã‚­ã‚¹ãƒˆã¯ä¸€åˆ‡æ›¸ã‹ãšã€å¿…ãšç´”ç²‹ãª JSON ãƒ†ã‚­ã‚¹ãƒˆã ã‘ã‚’è¿”ã—ã¦ãã ã•ã„ï¼ˆã‚³ãƒ¼ãƒ‰ãƒ•ã‚§ãƒ³ã‚¹ã‚„èª¬æ˜ã‚’å«ã‚ãªã„ã§ãã ã•ã„ï¼‰ã€‚

JSON ã‚¹ã‚­ãƒ¼ãƒä¾‹:
{
    "result": "ã€‡|â–³|Ã—",
    "evidence": [
        {"source": "ãƒ•ã‚¡ã‚¤ãƒ«åã‚„è­˜åˆ¥å­", "excerpt": "æŠœç²‹ãƒ†ã‚­ã‚¹ãƒˆ..."}
    ],
    "details": "è¿½åŠ ã®èª¬æ˜(ä»»æ„)"
}

é‡è¦: JSON ã®ã‚­ãƒ¼åã¯è‹±èªã®ã¾ã¾ã«ã—ã€å€¤ã‚„èª¬æ˜æ–‡ã¯æ—¥æœ¬èªã§è¨˜è¼‰ã—ã¦ãã ã•ã„ã€‚
"""

    init_human = HumanMessage(content=f"ãƒ«ãƒ¼ãƒ«è¦ç´„:\n{rule_summary}\n\nå…ƒãƒ«ãƒ¼ãƒ«(raw):\n{json.dumps(rule_raw, ensure_ascii=False)}\n\nãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ:\n{context}")

    messages = [SystemMessage(content=strict_prompt), init_human]
    resp = llm(messages)
    text = resp.content

    logger.debug("Agent B raw output (head 1000 chars): %s", text[:1000].replace('\n', '\\n'))

    def _save_model_output(rule_id: str, content: str):
        try:
            logs_dir = BASE_DIR / "logs"
            logs_dir.mkdir(exist_ok=True)
            ts = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
            fname = logs_dir / f"agent_b_output_{ts}_{str(rule_id)[:60].replace(' ', '_')}.log"
            with open(fname, "w", encoding="utf-8") as lf:
                lf.write("--- RAW MODEL OUTPUT ---\n")
                lf.write(content)
            logger.info("ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‚’ãƒ­ã‚°ã«ä¿å­˜ã—ã¾ã—ãŸ: %s", fname)
        except Exception as e:
            logger.debug("ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ãƒ­ã‚°ä¿å­˜ã«å¤±æ•—ã—ã¾ã—ãŸ: %s", e)

    if os.environ.get("SAVE_MODEL_OUTPUT", "1") != "0":
        try:
            _save_model_output(rule_raw.get("id") or rule_raw.get("title") or "unknown", text)
        except Exception:
            pass

    parsed = None
    try:
        parsed = json.loads(text)
    except Exception:
        parsed = None

    retries = 0
    while parsed is None and retries < 2:
        retries += 1
        logger.info("JSON ãƒ‘ãƒ¼ã‚¹å¤±æ•—: ãƒ¢ãƒ‡ãƒ«ã¸å†è©¦è¡Œã‚’è¡Œã„ã¾ã™ (è©¦è¡Œ %d)ã€‚", retries)
        followup = (
            "å‰ã®å›ç­”ã¯æœ‰ç”¨ã§ã—ãŸãŒã€è¦æ±‚ã•ã‚ŒãŸé€šã‚Šå³å¯†ãª JSON ã®ã¿ã§å‡ºåŠ›ã•ã‚Œã¦ã„ã¾ã›ã‚“ã§ã—ãŸã€‚"
            "ä»¥ä¸‹ã® JSON ã‚¹ã‚­ãƒ¼ãƒã«å³å¯†ã«åˆã‚ã›ã€ç´”ç²‹ãª JSON ãƒ†ã‚­ã‚¹ãƒˆã®ã¿ã‚’å‡ºåŠ›ã—ã¦ãã ã•ã„ã€‚"
            "\n\nã‚¹ã‚­ãƒ¼ãƒ: {\"result\":\"ã€‡|â–³|Ã—\", \"evidence\": [ {\"source\":..., \"excerpt\":...} ], \"details\": \"ä»»æ„ã®æ–‡å­—åˆ—\" }"
            "\n\nå…ƒã®å‡ºåŠ›ã‚’å‚ç…§ã—ã¦ã€ä¸Šè¨˜ã‚¹ã‚­ãƒ¼ãƒã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¦ JSON ã®ã¿ã‚’è¿”ã—ã¦ãã ã•ã„ã€‚"
        )
        follow_messages = [SystemMessage(content=strict_prompt), init_human, HumanMessage(content=followup + "\n\nå‰ã®å‡ºåŠ›:\n" + text)]
        try:
            resp2 = llm(follow_messages)
            text2 = resp2.content
            logger.debug("Agent B retry raw output (head 1000): %s", text2[:1000].replace('\n', '\\n'))
            try:
                parsed = json.loads(text2)
                text = text2
                break
            except Exception:
                m2 = re.search(r"(\{[\s\S]*\})", text2)
                if m2:
                    try:
                        parsed = json.loads(m2.group(1))
                        text = m2.group(1)
                        break
                    except Exception:
                        parsed = None
                t2 = text2.replace("'", '"')
                t2 = re.sub(r",\s*([}\]])", r"\1", t2)
                t2 = re.sub(r'([\{,\s])(\w+)\s*:', r'\1"\2":', t2)
                try:
                    parsed = json.loads(t2)
                    text = t2
                    break
                except Exception:
                    parsed = None
        except Exception as e:
            logger.debug("ãƒ¢ãƒ‡ãƒ«å†è©¦è¡Œä¸­ã«ä¾‹å¤–: %s", e)
            parsed = None

    if parsed is None:
        m = re.search(r"(\{[\s\S]*\})", text)
        if m:
            candidate = m.group(1)
            try:
                parsed = json.loads(candidate)
            except Exception:
                parsed = None

        if parsed is None:
            t2 = text.replace("'", '"')
            t2 = re.sub(r",\s*([}\]])", r"\1", t2)
            t2 = re.sub(r'([\{,\s])(\w+)\s*:', r'\1"\2":', t2)
            try:
                parsed = json.loads(t2)
            except Exception:
                parsed = None

    if parsed is None:
        logger.warning("ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ã‚’ JSON ã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚ãƒ’ãƒ¥ãƒ¼ãƒªã‚¹ãƒ†ã‚£ãƒƒã‚¯æŠ½å‡ºã‚’è©¦ã¿ã¾ã™ã€‚")

        def _heuristic_parse(text: str) -> Dict[str, Any]:
            out: Dict[str, Any] = {}
            m = re.search(r"['\"]?result['\"]?\s*[:ï¼š]\s*['\"]?([^\"',}\n\r]+)", text, re.IGNORECASE)
            if m:
                out["result"] = m.group(1).strip().strip('"\'')
            else:
                m2 = re.search(r"\b(ã€‡|â–³|Ã—|O|X|o|x)\b", text)
                if m2:
                    out["result"] = m2.group(1)

            m_e = re.search(r"['\"]?evidence['\"]?\s*[:ï¼š]\s*([\"'])(.*?)\1", text, re.IGNORECASE | re.DOTALL)
            if m_e:
                out["evidence"] = m_e.group(2).strip()
            else:
                m_e2 = re.search(r"evidence\s*[:ï¼š\-]\s*(.+)$", text, re.IGNORECASE | re.DOTALL)
                if m_e2:
                    out["evidence"] = m_e2.group(1).strip()

            m_d = re.search(r"['\"]?details['\"]?\s*[:ï¼š]\s*([\"'])(.*?)\1", text, re.IGNORECASE | re.DOTALL)
            if m_d:
                out["details"] = m_d.group(2).strip()
            else:
                m_d2 = re.search(r"details\s*[:ï¼š\-]\s*(.+)$", text, re.IGNORECASE | re.DOTALL)
                if m_d2:
                    out["details"] = m_d2.group(1).strip()

            if not out.get("evidence") and text:
                out["evidence"] = text.strip()

            if "result" not in out:
                out["result"] = "â–³"

            return out

        parsed = _heuristic_parse(text)

    def _build_evidence_list(evidence_field, docs_list: List[Document]):
        evs = []
        if not evidence_field:
            for d in docs_list[:TOP_K]:
                evs.append({"source": d.metadata.get("source"), "excerpt": d.page_content[:400].strip()})
            return evs

        if isinstance(evidence_field, str):
            evs.append({"source": "(model-output)", "excerpt": evidence_field})
            for d in docs_list[:TOP_K]:
                evs.append({"source": d.metadata.get("source"), "excerpt": d.page_content[:300].strip()})
            return evs

        if isinstance(evidence_field, list):
            for item in evidence_field:
                if isinstance(item, dict):
                    src = item.get("source") or item.get("file") or item.get("path") or "(unknown)"
                    exc = item.get("excerpt") or item.get("text") or json.dumps(item, ensure_ascii=False)
                    evs.append({"source": src, "excerpt": exc[:400].strip()})
                else:
                    evs.append({"source": "(model-output)", "excerpt": str(item)[:400]})
            return evs

        evs.append({"source": "(model-output)", "excerpt": str(evidence_field)[:400]})
        return evs

    parsed_evidence = _build_evidence_list(parsed.get("evidence"), docs)
    parsed["evidence_normalized"] = parsed_evidence
    return parsed


def format_b_result(b_result: Dict[str, Any]) -> str:
    """Agent B ã®æ§‹é€ åŒ–çµæœã‚’æ—¥æœ¬èªã®æ•´å½¢ãƒ†ã‚­ã‚¹ãƒˆã«å¤‰æ›ã™ã‚‹"""
    lines: List[str] = []
    res = b_result.get("result") or b_result.get("status") or "â–³"
    lines.append(f"åˆ¤å®š: {res}")
    
    details = b_result.get("details") or b_result.get("detail") or b_result.get("notes")
    if details:
        lines.append("\nèª¬æ˜:")
        if isinstance(details, str):
            lines.append(details)
        else:
            lines.append(json.dumps(details, ensure_ascii=False, indent=2))

    evs = b_result.get("evidence_normalized") or []
    if evs:
        lines.append("\næ ¹æ‹  (å‚ç…§æ–‡æ›¸ã¨æŠœç²‹):")
        for i, e in enumerate(evs, 1):
            src = e.get("source") or "(unknown)"
            excerpt = e.get("excerpt") or ""
            excerpt_clean = excerpt.replace("\n", " ").strip()
            if len(excerpt_clean) > 1200:
                excerpt_clean = excerpt_clean[:1200].rstrip() + " ..."
            lines.append(f"  {i}. source: {src}")
            wrapped = textwrap.fill(excerpt_clean, width=100, subsequent_indent='     ')
            lines.append(textwrap.indent(wrapped, '     '))

    if b_result.get("evidence") and not evs:
        lines.append("\nãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ï¼ˆæ ¹æ‹ ï¼‰:")
        lines.append(str(b_result.get("evidence")))

    return "\n".join(lines)


def find_rule_by_query(rules: List[Dict[str, Any]], query: str) -> Optional[Dict[str, Any]]:
    """ãƒ«ãƒ¼ãƒ«ä¸€è¦§ã‹ã‚‰ query ã‚’å…ƒã«ãƒ«ãƒ¼ãƒ«ã‚’æ¤œç´¢ã™ã‚‹ã€‚ID/ãƒ‘ã‚¹/ã‚¿ã‚¤ãƒˆãƒ«/æœ¬æ–‡ã®éƒ¨åˆ†ä¸€è‡´ã§æœ€åˆã®ãƒãƒƒãƒã‚’è¿”ã™ã€‚"""
    q = query.strip().lower()
    for r in rules:
        if r.get("id") and str(r.get("id")).lower() == q:
            return r
    for r in rules:
        if q in (r.get("title") or "").lower() or q in (r.get("path") or "").lower() or q in (r.get("content") or "").lower():
            return r
    return None


def run_cui_mode(rules: List[Dict[str, Any]], vectordb: Chroma):
    """CUI ãƒ™ãƒ¼ã‚¹ã®ç°¡æ˜“ãƒãƒ£ãƒƒãƒˆãƒ«ãƒ¼ãƒ—"""
    llm = make_chat_model()

    help_text = (
        "ã‚³ãƒãƒ³ãƒ‰ä¸€è¦§:\n"
        "  help                      ãƒ˜ãƒ«ãƒ—è¡¨ç¤º\n"
        "  list                      èª­ã¿è¾¼ã‚“ã ãƒ«ãƒ¼ãƒ«ä¸€è¦§ã®ä¸€éƒ¨ã‚’è¡¨ç¤º\n"
        "  show <query>              ãƒ«ãƒ¼ãƒ«ã‚’è¡¨ç¤ºï¼ˆid/title ã®éƒ¨åˆ†ä¸€è‡´ï¼‰\n"
        "  check <query>             æŒ‡å®šã—ãŸãƒ«ãƒ¼ãƒ«ã«å¯¾ã—ã¦ã‚·ã‚¹ãƒ†ãƒ ãŒå¾“ã£ã¦ã„ã‚‹ã‹è©•ä¾¡ï¼ˆA->B ã®é †ï¼‰\n"
        "  showfull <summary|b>      ç›´è¿‘ã®ãƒã‚§ãƒƒã‚¯ã§ä¿å­˜ã•ã‚ŒãŸé …ç›®ã®å…¨æ–‡è¡¨ç¤º\n"
        "  ask <è‡ªç”±ãƒ†ã‚­ã‚¹ãƒˆ>        ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã«é–¢ã™ã‚‹ RAG è³ªå•\n"
        "  quit                      çµ‚äº†\n"
    )

    print("AgentRAG ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (CUI)ã€‚help ã¨å…¥åŠ›ã—ã¦ãã ã•ã„ã€‚\n")
    last_store: Dict[str, Any] = {"summary": None, "b": None}

    def print_section(title: str, content: str, max_len: int = 1200):
        """è¦‹ã‚„ã™ã„ã‚»ã‚¯ã‚·ãƒ§ãƒ³è¡¨ç¤º"""
        sep = "=" * 80
        print("\n" + sep)
        print(f"{title}")
        print(sep)
        if content is None:
            print("(ãªã—)\n")
            return

        display_text = content
        if isinstance(content, str) and len(content) > max_len:
            display_text = content[:max_len].rstrip() + "\n...ï¼ˆå…¨æ–‡ã¯ 'showfull' ã‚³ãƒãƒ³ãƒ‰ã§è¡¨ç¤ºå¯ï¼‰"

        if isinstance(display_text, str) and display_text.strip().startswith(("{", "[")):
            print(display_text)
        else:
            if isinstance(display_text, str):
                paras = [p.strip() for p in display_text.split("\n\n") if p.strip()]
                for p in paras:
                    wrapped = textwrap.fill(p, width=100)
                    print(wrapped)
                    print()
            else:
                print(str(display_text))
        print(sep + "\n")

    while True:
        try:
            cmd = input("> ").strip()
        except (EOFError, KeyboardInterrupt):
            print("\nçµ‚äº†ã—ã¾ã™ã€‚")
            break

        if not cmd:
            continue
        if cmd == "help":
            print(help_text)
            continue
        if cmd == "list":
            for i, r in enumerate(rules[:50], 1):
                print(f"{i}. id={r.get('id')} title={r.get('title')} path={r.get('path')}")
            continue
        if cmd.startswith("show "):
            q = cmd[len("show "):].strip()
            r = find_rule_by_query(rules, q)
            if not r:
                print("ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸã€‚éƒ¨åˆ†æ–‡å­—åˆ—ã§æ¤œç´¢ã—ã¦ã¿ã¦ãã ã•ã„ã€‚")
            else:
                print("--- ãƒ«ãƒ¼ãƒ« ---")
                print(f"id: {r.get('id')}")
                print(f"title: {r.get('title')}")
                print(f"path: {r.get('path')}")
                print("content:")
                print(r.get("content"))
            continue

        if cmd.startswith("check "):
            q = cmd[len("check "):].strip()
            r = find_rule_by_query(rules, q)
            if not r:
                print("ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚åˆ¥ã®ã‚¯ã‚¨ãƒªã‚’è©¦ã—ã¦ãã ã•ã„ã€‚\n(ä¾‹: ãƒ«ãƒ¼ãƒ«ã®ä¸€éƒ¨ã®èªå¥ã‚„ id ã‚’å…¥åŠ›)\n")
                continue

            print(f"é¸æŠã•ã‚ŒãŸãƒ«ãƒ¼ãƒ«: {r.get('title')} (path: {r.get('path')})")

            # Agent A: è¦ç´„
            rule_text = r.get("content") or ""
            related = retrieve_related_docs(vectordb, rule_text, k=TOP_K)
            print("[Agent A] ãƒ«ãƒ¼ãƒ«ã¨é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‹ã‚‰è¦ç´„ã‚’ä½œæˆã—ã¦ã„ã¾ã™...")
            summary = agent_a_summarize(llm, rule_text, related)
            last_store["summary"] = summary
            print_section("Agent A - è¦ç´„ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼", summary)

            # Agent B: ç¢ºèª
            print("[Agent B] ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒãƒ«ãƒ¼ãƒ«ã«å¾“ã£ã¦ã„ã‚‹ã‹è©•ä¾¡ã—ã¦ã„ã¾ã™...")
            b_result = agent_b_check(llm, summary, r.get("raw", {}), related)
            last_store["b"] = b_result
            
            b_preview_text = format_b_result(b_result)
            print_section("Agent B - åˆ¤å®šï¼ˆãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ï¼‰", b_preview_text)
            
            evs = b_result.get("evidence_normalized") or []
            if evs:
                print("æ ¹æ‹  (å‚ç…§æ–‡æ›¸ã¨æŠœç²‹):")
                for i, e in enumerate(evs, 1):
                    src = e.get("source") or "(unknown)"
                    excerpt = e.get("excerpt") or ""
                    print(f"  {i}. source: {src}")
                    excerpt_clean = excerpt.replace("\n", " ").strip()
                    if len(excerpt_clean) > 1000:
                        excerpt_clean = excerpt_clean[:1000].rstrip() + " ..."
                    wrapped = textwrap.fill(excerpt_clean, width=100, subsequent_indent='     ')
                    print(textwrap.indent(wrapped, '     '))
                    print()
            else:
                print("(æ ¹æ‹ æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“)")
            
            result_symbol = b_result.get("result", "â–³")
            if result_symbol == "ã€‡" or result_symbol == "O" or result_symbol == "o":
                print("è£œè¶³: åˆ¤å®šã¯ 'å¾“ã£ã¦ã„ã‚‹' ã¨è¦‹ãªã•ã‚Œã¾ã™ã€‚å¿…è¦ã«å¿œã˜ã¦é–¢é€£è³‡æ–™ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n")
            elif result_symbol == "Ã—" or result_symbol == "X" or result_symbol == "x":
                print("è£œè¶³: åˆ¤å®šã¯ 'å¾“ã£ã¦ã„ãªã„' ã§ã™ã€‚å„ªå…ˆçš„ãªå¯¾å¿œï¼ˆä¿®æ­£ï¼è¨­å®šå¤‰æ›´ç­‰ï¼‰ãŒå¿…è¦ã§ã™ã€‚è©³ç´°ã¯é–¢é€£è³‡æ–™ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n")
            else:
                print("è£œè¶³: åˆ¤å®šã¯ 'â–³'ï¼ˆè¿½åŠ ç¢ºèªãŒå¿…è¦ï¼‰ã§ã™ã€‚é–¢é€£ç®‡æ‰€ã®ãƒ­ã‚°ã‚„è¨­å®šãƒ•ã‚¡ã‚¤ãƒ«ã‚’è¿½åŠ ã§æä¾›ã—ã¦ãã ã•ã„ã€‚\n")

            print("è©•ä¾¡ãƒ•ãƒ­ãƒ¼ãŒå®Œäº†ã—ã¾ã—ãŸã€‚å¿…è¦ã«å¿œã˜ã¦ 'showfull summary' ã‚„ 'showfull b' ã§å…¨æ–‡ã‚’è¡¨ç¤ºã§ãã¾ã™ã€‚")
            continue

        if cmd.startswith("showfull "):
            what = cmd[len("showfull "):].strip()
            if what not in ("summary", "b"):
                print("'showfull' ã®å¼•æ•°ã¯ summary|b ã®ã„ãšã‚Œã‹ã‚’æŒ‡å®šã—ã¦ãã ã•ã„ã€‚")
                continue
            val = last_store.get(what)
            if val is None:
                print(f"ã¾ã  '{what}' ã®å‡ºåŠ›ãŒã‚ã‚Šã¾ã›ã‚“ã€‚å…ˆã« 'check <query>' ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
                continue
            title_map = {"summary": "Agent A - è¦ç´„ï¼ˆå…¨æ–‡ï¼‰", "b": "Agent B - åˆ¤å®šï¼ˆå…¨æ–‡ï¼‰"}
            
            if isinstance(val, str):
                content = val
            else:
                if what == "b":
                    content = format_b_result(val)
                else:
                    content = json.dumps(val, ensure_ascii=False, indent=2)
            print_section(title_map.get(what, what), content, max_len=10_000)
            continue

        if cmd.startswith("ask "):
            q = cmd[len("ask "):].strip()
            docs = retrieve_related_docs(vectordb, q, k=TOP_K)
            context = "\n\n".join([f"[src:{d.metadata.get('source')}]\n{d.page_content}" for d in docs])
            system = "ã‚ãªãŸã¯ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«ã€é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚"
            messages = [SystemMessage(content=system), HumanMessage(content=f"è³ªå•: {q}\n\nå‚ç…§æ–‡æ›¸:\n{context}")]
            resp = llm(messages)
            print(resp.content)
            continue

        if cmd in ("quit", "exit", "q"):
            print("çµ‚äº†ã—ã¾ã™ã€‚")
            break

        print("ä¸æ˜ãªã‚³ãƒãƒ³ãƒ‰ã§ã™ã€‚help ã‚’è¡¨ç¤ºã—ã¦ãã ã•ã„ã€‚")


def run_streamlit_mode():
    """Streamlit WebUI ãƒ¢ãƒ¼ãƒ‰ã‚’å®Ÿè¡Œ"""
    try:
        import streamlit as st
    except ImportError:
        print("Streamlit ãŒã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚pip install streamlit ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ã€‚")
        sys.exit(1)
        
    # Streamlit UI ã®å®Ÿè£…
    st.set_page_config(page_title="AgentRAG - Web UI", layout="wide")

    st.markdown("# ğŸ›¡ï¸ AgentRAG â€” ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦å‰‡ãƒã‚§ãƒƒã‚«ãƒ¼")
    st.markdown("*çµ±ä¸€åŸºæº–å¯¾å¿œ RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ (Streamlit UI)*")
    st.markdown("---")

    # CSS ã‚¹ã‚¿ã‚¤ãƒ«
    st.markdown(
        """
        <style>
        * { font-size:13px !important; }
        .stButton>button { padding:4px 8px !important; font-size:13px !important; }
        textarea { font-size:12px !important; }
        
        .main .block-container {
            padding-top: 3rem !important;
            padding-left: 1rem !important;
            padding-right: 1rem !important;
            padding-bottom: 5rem !important;
            max-width: none !important;
            overflow-y: visible !important;
        }
        
        .main h1 {
            margin-top: 0 !important;
            margin-bottom: 0.5rem !important;
            padding-top: 0 !important;
            font-size: 1.8rem !important;
            line-height: 1.2 !important;
        }
        
        .main em {
            font-size: 0.9rem !important;
            color: #666 !important;
            display: block !important;
            margin-bottom: 0.5rem !important;
        }
        
        .main hr {
            margin: 0.5rem 0 1rem 0 !important;
        }
        
        html, body, #root {
            overflow-y: auto !important;
            height: 100% !important;
        }
        
        .main {
            overflow-y: auto !important;
            height: 100vh !important;
            padding-top: 0 !important;
        }
        
        .stSelectbox div[data-baseweb="select"] > div {
            max-height: 300px !important; 
            overflow-y: auto !important;
        }
        
        div[data-baseweb="popover"] {
            max-height: 400px !important;
            overflow-y: auto !important;
        }
        
        header[data-testid="stHeader"] {
            height: 2.5rem !important;
        }
        
        .css-1d391kg {
            padding-top: 1rem !important;
        }
        
        @media (max-width: 768px) {
            .main h1 {
                font-size: 1.5rem !important;
            }
            .main .block-container {
                padding-left: 0.5rem !important;
                padding-right: 0.5rem !important;
            }
        }
        
        @media (max-width: 480px) {
            .main h1 {
                font-size: 1.3rem !important;
            }
            * {
                font-size: 12px !important;
            }
        }
        </style>
        """,
        unsafe_allow_html=True,
    )

    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä»˜ãåˆæœŸåŒ–
    @st.cache_resource
    def get_vectordb():
        docs = load_spec_documents(SPEC_DIR)
        return init_chroma(docs)

    @st.cache_resource
    def get_rules():
        return load_rules_from_dir(RULE_DIR)

    @st.cache_resource
    def get_llm():
        return make_chat_model()

    vectordb = None
    llm = None
    rules = []
    try:
        vectordb = get_vectordb()
        llm = get_llm()
        rules = get_rules()
    except Exception as e:
        st.warning("ãƒ™ã‚¯ãƒˆãƒ«DB ã‚„ LLM ã®åˆæœŸåŒ–ã§è­¦å‘ŠãŒå‡ºã¾ã—ãŸã€‚OpenAIã‚­ãƒ¼ã‚„ä¾å­˜ãŒæ­£ã—ãè¨­å®šã•ã‚Œã¦ã„ã‚‹ã‹ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
        st.exception(e)

    # ã‚µã‚¤ãƒ‰ãƒãƒ¼
    with st.sidebar:
        st.markdown("### ğŸ“‹ ãƒ¡ãƒ‹ãƒ¥ãƒ¼")
        st.write(f"**ãƒ«ãƒ¼ãƒ«æ•°**: {len(rules):,}ä»¶")
        
        page = st.radio("ãƒšãƒ¼ã‚¸é¸æŠ", ["ğŸ” ãƒ«ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯", "ğŸ’¬ RAG è³ªå•"], label_visibility="collapsed")
        
        st.markdown("---")
        st.markdown("### âš™ï¸ è¨­å®š")
        topk = st.slider("å‚ç…§ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°", 1, 10, TOP_K, help="RAGæ¤œç´¢ã§å‚ç…§ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®æ•°")
        
        st.markdown("---")
        st.markdown("### ğŸ“ å¯¾å¿œãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼")
        st.markdown("""
        **ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿å¯¾å¿œ:**
        - ğŸ“„ PDF (.pdf)
        - ğŸ“ Word (.docx)
        - ğŸ“Š Excel (.xlsx)
        - ğŸ“ˆ PowerPoint (.pptx)
        - ğŸ“‹ Markdown (.md)
        - ğŸ“„ Text (.txt)
        
        **ãƒ«ãƒ¼ãƒ«å®šç¾©:**
        - ğŸ“‹ JSON (.json)
        """)
        
        st.markdown("---")
        st.caption("ğŸ’¡ `specification/` ãƒ•ã‚©ãƒ«ãƒ€ã«ãƒ•ã‚¡ã‚¤ãƒ«ã‚’é…ç½®ã—ã¦ãã ã•ã„")

    # ãƒ«ãƒ¼ãƒ«ä¸€è¦§ã‚’å–å¾—
    def create_rule_preview(rule):
        """ãƒ«ãƒ¼ãƒ«ã®é¸æŠè‚¢ç”¨ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãƒ†ã‚­ã‚¹ãƒˆã‚’ä½œæˆ"""
        rule_id = rule.get('id', '')
        title = rule.get('title', '')
        rule_type = rule.get('type', '')
        content = rule.get('content', '')
        
        preview_content = content.replace('\n', ' ').replace('\r', '').strip()
        if len(preview_content) > 50:
            preview_content = preview_content[:50] + "..."
        
        choice_text = f"{rule_id}"
        if rule_type:
            choice_text += f" [{rule_type}]"
        
        if title and title != rule_id:
            short_title = title[:30] + "..." if len(title) > 30 else title
            choice_text += f" {short_title}"
        
        if preview_content:
            choice_text += f" | {preview_content}"
        
        return choice_text

    rule_choices = {}
    for r in rules:
        preview_text = create_rule_preview(r)
        rule_choices[preview_text] = r

    if "ãƒ«ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯" in page:
        st.header("ğŸ” ãƒ«ãƒ¼ãƒ«ãƒã‚§ãƒƒã‚¯")
        
        # æ¤œç´¢æ©Ÿèƒ½
        col1, col2 = st.columns([3, 1])
        with col1:
            search_term = st.text_input("ãƒ«ãƒ¼ãƒ«æ¤œç´¢ï¼ˆIDã€ç¨®åˆ¥ã€å†…å®¹ã§æ¤œç´¢ï¼‰", placeholder="ä¾‹: è²¬ä»»è€…, éµå®ˆäº‹é …, 2.1.1")
        with col2:
            st.write("")
            show_all = st.checkbox("å…¨ä»¶è¡¨ç¤º", help="ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã¨æ¤œç´¢çµæœã®å…¨ä»¶ã‚’è¡¨ç¤ºã—ã¾ã™ï¼ˆé‡ã„å ´åˆãŒã‚ã‚Šã¾ã™ï¼‰")
        
        max_display_items = 500 if show_all else 100
        
        # æ¤œç´¢ã§ãƒ•ã‚£ãƒ«ã‚¿ãƒªãƒ³ã‚°
        filtered_choices = {}
        
        if search_term:
            count = 0
            for preview_text, rule in rule_choices.items():
                if count >= max_display_items:
                    break
                if (search_term.lower() in preview_text.lower() or
                    search_term.lower() in rule.get('content', '').lower() or
                    search_term.lower() in rule.get('id', '').lower() or
                    search_term.lower() in rule.get('type', '').lower()):
                    filtered_choices[preview_text] = rule
                    count += 1
            
            if filtered_choices:
                total_matches = sum(1 for preview_text, rule in rule_choices.items() 
                                  if (search_term.lower() in preview_text.lower() or
                                      search_term.lower() in rule.get('content', '').lower() or
                                      search_term.lower() in rule.get('id', '').lower() or
                                      search_term.lower() in rule.get('type', '').lower()))
                
                if total_matches > max_display_items:
                    st.info(f"ğŸ” æ¤œç´¢çµæœ: {total_matches}ä»¶ä¸­ ä¸Šä½{len(filtered_choices)}ä»¶ã‚’è¡¨ç¤º")
                    if not show_all:
                        st.caption("ã‚ˆã‚Šå¤šãè¡¨ç¤ºã™ã‚‹ã«ã¯ã€Œå…¨ä»¶è¡¨ç¤ºã€ã‚’ãƒã‚§ãƒƒã‚¯ã™ã‚‹ã‹ã€æ¤œç´¢èªã‚’å…·ä½“åŒ–ã—ã¦ãã ã•ã„")
                else:
                    st.success(f"ğŸ” æ¤œç´¢çµæœ: {len(filtered_choices)}ä»¶ã®ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ")
            else:
                st.warning("ğŸ” æ¤œç´¢æ¡ä»¶ã«ä¸€è‡´ã™ã‚‹ãƒ«ãƒ¼ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã§ã—ãŸ")
        else:
            count = 0
            for preview_text, rule in rule_choices.items():
                if count >= max_display_items:
                    break
                filtered_choices[preview_text] = rule
                count += 1
            
            if len(rule_choices) > max_display_items:
                st.info(f"ğŸ“‹ å…¨{len(rule_choices)}ä»¶ä¸­ ä¸Šä½{max_display_items}ä»¶ã‚’è¡¨ç¤º")
                st.caption("æ¤œç´¢æ©Ÿèƒ½ã¾ãŸã¯ã€Œå…¨ä»¶è¡¨ç¤ºã€ãƒã‚§ãƒƒã‚¯ã§ä»–ã®ãƒ«ãƒ¼ãƒ«ã‚‚è¡¨ç¤ºã§ãã¾ã™")
        
        # ãƒ«ãƒ¼ãƒ«é¸æŠ
        with st.container():
            choices = ["(é¸æŠã—ã¦ãã ã•ã„)"] + list(filtered_choices.keys())
            sel = st.selectbox(
                "è©•ä¾¡ã™ã‚‹ãƒ«ãƒ¼ãƒ«ã‚’é¸æŠ", 
                choices, 
                help="ãƒ«ãƒ¼ãƒ«IDã€ç¨®åˆ¥ã€å†…å®¹ã®ãƒ—ãƒ¬ãƒ“ãƒ¥ãƒ¼ãŒè¡¨ç¤ºã•ã‚Œã¾ã™",
                key="rule_selector"
            )
        
        st.caption("âš™ï¸ ãƒ«ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ 'ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ' ã‚’æŠ¼ã™ã¨è©•ä¾¡ãŒå§‹ã¾ã‚Šã¾ã™ã€‚")

        if sel == "(é¸æŠã—ã¦ãã ã•ã„)":
            if search_term:
                st.info("ä¸Šè¨˜ã®æ¤œç´¢çµæœã‹ã‚‰ãƒ«ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„")
            else:
                st.info("ãƒ«ãƒ¼ãƒ«ã‚’é¸æŠã—ã¦ãã ã•ã„ï¼ˆä¸Šéƒ¨ã®æ¤œç´¢ãƒœãƒƒã‚¯ã‚¹ã§çµã‚Šè¾¼ã¿å¯èƒ½ï¼‰")
        else:
            r = filtered_choices[sel]
            
            col1, col2 = st.columns([1, 1])
            
            with col1:
                st.markdown(f"**ID**: `{r.get('id')}`")
                if r.get('type'):
                    st.markdown(f"**ç¨®åˆ¥**: {r.get('type')}")
                if r.get('source_file'):
                    st.markdown(f"**ã‚½ãƒ¼ã‚¹**: {r.get('source_file')}")
            
            with col2:
                st.markdown(f"**ã‚¿ã‚¤ãƒˆãƒ«**: {r.get('title')}")
                if r.get('path'):
                    st.markdown(f"**éšå±¤**: `{r.get('path')}`")
            
            content = r.get('content', '')
            if content:
                st.markdown("**å†…å®¹:**")
                if len(content) > 500:
                    with st.expander(f"å†…å®¹ã‚’è¡¨ç¤ºï¼ˆ{len(content)}æ–‡å­—ï¼‰"):
                        st.text(content)
                    st.text(content[:200] + "..." if len(content) > 200 else content)
                else:
                    st.text(content)

            if st.button("ãƒã‚§ãƒƒã‚¯å®Ÿè¡Œ"):
                try:
                    docs = retrieve_related_docs(vectordb, r.get('content') or r.get('title') or "", k=topk)
                    st.write(f"å–å¾—ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ: {len(docs)} ãƒãƒ£ãƒ³ã‚¯ï¼ˆä¸Šä½ {topk}ï¼‰")
                    st.info("è¦ç´„ä¸­...")
                    summary = agent_a_summarize(llm, r.get('content') or '', docs)
                    st.success("è¦ç´„å®Œäº†")

                    st.info("è©•ä¾¡ä¸­...")
                    b_result = agent_b_check(llm, summary, r.get('raw', {}), docs)
                    st.success("è©•ä¾¡å®Œäº†")
                    st.subheader("åˆ¤å®šï¼ˆAgent Bï¼‰")
                    
                    b_text = format_b_result(b_result)
                    def _normalize_display(text: str) -> str:
                        if not text:
                            return ""
                        t = text.replace('\r\n', '\n').replace('\r', '\n')
                        t = re.sub(r"\n{3,}", "\n\n", t)
                        lines = [ln.rstrip() for ln in t.split('\n')]
                        while lines and lines[0].strip() == "":
                            lines.pop(0)
                        while lines and lines[-1].strip() == "":
                            lines.pop()
                        out_lines = []
                        prev_blank = False
                        for ln in lines:
                            if ln.strip() == "":
                                if not prev_blank:
                                    out_lines.append("")
                                prev_blank = True
                            else:
                                out_lines.append(ln.lstrip())
                                prev_blank = False
                        return "\n".join(out_lines)

                    b_text_clean = _normalize_display(b_text)
                    res_symbol = b_result.get("result") or b_result.get("status") or "â–³"
                    st.markdown(f"**åˆ¤å®š: {res_symbol}**")

                    details = b_result.get("details") or b_result.get("detail") or b_result.get("notes")
                    if details:
                        st.markdown("**èª¬æ˜:**")
                        st.text(details if isinstance(details, str) else json.dumps(details, ensure_ascii=False, indent=2))

                    evs = b_result.get("evidence_normalized") or []
                    if evs:
                        st.markdown("**æ ¹æ‹  (å‚ç…§æ–‡æ›¸ã¨æŠœç²‹):**")
                        for i, e in enumerate(evs, 1):
                            src = e.get("source") or "(unknown)"
                            excerpt = e.get("excerpt") or ""
                            with st.expander(f"{i}. {src}"):
                                ex = excerpt.replace("\r\n", "\n").replace("\r", "\n").strip()
                                st.text(ex)
                    else:
                        st.info("(æ ¹æ‹ æƒ…å ±ã¯ã‚ã‚Šã¾ã›ã‚“)")

                    with st.expander("ï¼ˆå‚è€ƒï¼‰æ•´å½¢æ¸ˆã¿ãƒ†ã‚­ã‚¹ãƒˆï¼ˆç”Ÿï¼‰"):
                        st.text(b_text_clean)

                except Exception as e:
                    st.error("è©•ä¾¡ã«å¤±æ•—ã—ã¾ã—ãŸã€‚ãƒ­ã‚°ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚")
                    st.exception(e)

    elif "RAG" in page:
        st.header("ğŸ’¬ RAG è³ªå• (ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã«é–¢ã™ã‚‹ QA)")
        st.caption("ğŸ“ PDF, Word, Excel, PowerPoint, Markdown, ãƒ†ã‚­ã‚¹ãƒˆãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æƒ…å ±ã‚’æ¤œç´¢ã§ãã¾ã™")
        
        q = st.text_input("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„", placeholder="ä¾‹: ã‚¦ã‚¤ãƒ«ã‚¹å¯¾ç­–ã®è¦ä»¶ã¯ï¼Ÿ / Excelå½¢å¼ã®è¦ä»¶ã¯ï¼Ÿ")
        if st.button("è³ªå•å®Ÿè¡Œ"):
            if not q:
                st.warning("è³ªå•ã‚’å…¥åŠ›ã—ã¦ãã ã•ã„")
            else:
                try:
                    docs = retrieve_related_docs(vectordb, q, k=topk)
                    st.write(f"ğŸ” {len(docs)}ä»¶ã®é–¢é€£ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’æ¤œç´¢ã—ã¾ã—ãŸ")
                    
                    file_types_found = set()
                    for d in docs:
                        file_type = d.metadata.get('file_type', 'unknown')
                        file_types_found.add(file_type)
                    
                    if file_types_found:
                        type_emojis = {'.pdf': 'ğŸ“„', '.docx': 'ğŸ“', '.xlsx': 'ğŸ“Š', '.pptx': 'ğŸ“ˆ', '.md': 'ğŸ“‹', '.txt': 'ğŸ“„'}
                        type_str = " ".join([f"{type_emojis.get(ft, 'ğŸ“„')}{ft}" for ft in sorted(file_types_found)])
                        st.caption(f"å‚ç…§ãƒ•ã‚¡ã‚¤ãƒ«å½¢å¼: {type_str}")
                    
                    context = "\n\n".join([f"[src:{d.metadata.get('source')}]\n{d.page_content}" for d in docs])
                    system = "ã‚ãªãŸã¯ã‚·ã‚¹ãƒ†ãƒ æƒ…å ±ã®æ¤œç´¢ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã§ã™ã€‚ãƒ¦ãƒ¼ã‚¶ã®è³ªå•ã«ã€é–¢é€£ã™ã‚‹ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’å‚ç…§ã—ã¦ç°¡æ½”ã«ç­”ãˆã¦ãã ã•ã„ã€‚\n\né‡è¦: å›ç­”ã¯å¿…ãšæ—¥æœ¬èªã§è¡Œã£ã¦ãã ã•ã„ã€‚"
                    messages = [SystemMessage(content=system), HumanMessage(content=f"è³ªå•: {q}\n\nå‚ç…§æ–‡æ›¸:\n{context}")]
                    resp = llm(messages)
                    st.markdown("**å›ç­”:**")
                    st.text(resp.content)
                    
                    with st.expander("ğŸ”— å‚ç…§å…ƒãƒ•ã‚¡ã‚¤ãƒ«è©³ç´°"):
                        for i, d in enumerate(docs, 1):
                            source = d.metadata.get('source', 'unknown')
                            file_type = d.metadata.get('file_type', 'unknown')
                            chunk_id = d.metadata.get('chunk', 0)
                            st.text(f"{i}. {Path(source).name} ({file_type}, chunk {chunk_id})")
                            st.text(f"   å†…å®¹: {d.page_content[:100]}...")
                            st.text("")
                            
                except Exception as e:
                    st.error("QA å®Ÿè¡Œã§ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ")
                    st.exception(e)

    st.caption("ã“ã® UI ã¯ Streamlit ã‚’åˆ©ç”¨ã—ã¦ã„ã¾ã™ã€‚ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰ã§ã‚ã‚‹ main.py ã‚’å¤§ããå¤‰æ›´ã›ãšã«ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰ã‚’æä¾›ã—ã¾ã™ã€‚")


def main():
    parser = argparse.ArgumentParser(description="ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£è¦å‰‡ãƒã‚§ãƒƒã‚¯RAGãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆ")
    parser.add_argument("--cui", action="store_true", help="CUIãƒ¢ãƒ¼ãƒ‰ã§å®Ÿè¡Œ")
    args = parser.parse_args()
    
    # ãƒ«ãƒ¼ãƒ«èª­ã¿è¾¼ã¿
    rules = load_rules_from_dir(RULE_DIR)

    # ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆèª­ã¿è¾¼ã¿
    docs = load_spec_documents(SPEC_DIR)

    # Chroma åˆæœŸåŒ–ãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
    vectordb = init_chroma(docs)
    
    if args.cui:
        # CUIãƒ¢ãƒ¼ãƒ‰
        run_cui_mode(rules, vectordb)
    else:
        # Streamlitãƒ¢ãƒ¼ãƒ‰ãªã‚‰Streamlitã‚¢ãƒ—ãƒªã¨ã—ã¦å®Ÿè¡Œ
        # ã“ã®å ´åˆã¯æœ¬å½“ã®ã‚¨ãƒ³ãƒˆãƒªãƒ¼ãƒã‚¤ãƒ³ãƒˆã§ã¯ãªãã€
        # streamlit run combined_app.py ã§å®Ÿè¡Œã•ã‚Œã‚‹
        run_streamlit_mode()


if __name__ == "__main__":
    main()